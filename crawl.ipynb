{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To crawl https://www.gov.sg/factually/page-2 then \n",
    "https://www.gov.sg/factually/content/are-more-foreigners-taking-away-jobs-of-singaporeans\n",
    "\n",
    "reference https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post activity thoughts:\n",
    "The code should be written for robust-ness. I understand why json is one of the the preferred communciation format, and why dictionaries are useful. Even with missing information a dictionary-like file can still be created without hassle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for `threading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_articles_from_page(page_num, debug = False):\n",
    "    '''\n",
    "    given page_num\n",
    "    returns category_name_texts, \n",
    "            category_name_links, \n",
    "            article_title_links,\n",
    "            article_title_texts\n",
    "    '''\n",
    "    \n",
    "    category_name_texts = []\n",
    "#     category_name_links = []\n",
    "    article_title_texts = []\n",
    "    article_title_links = []\n",
    "    \n",
    "    if True:\n",
    "#     try:\n",
    "        page = requests.get('https://www.gov.sg/factually/page-{}'.format(page_num))\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "#         soup = soup.find(class_=\"content factually-section\")\n",
    "        # narrow down to this section\n",
    "#         print(soup)\n",
    "\n",
    "        grid_items = soup.find_all(class_=\"news-info\")\n",
    "#         print(grid_items)\n",
    "        for grid_item in grid_items:\n",
    "\n",
    "#             try:\n",
    "                category_item = grid_item.find(class_=\"category\")\n",
    "                category_name_text = category_item.text\n",
    "                category_name_text = category_name_text.strip()\n",
    "#                 category_name_link = category_item.get('href')\n",
    "\n",
    "                article_title_item = grid_item.find('h2')\n",
    "                article_title_text = article_title_item.text\n",
    "                article_title_link = article_title_item.find('a').get('href')\n",
    "\n",
    "                category_name_texts.append(category_name_text)\n",
    "#                 category_name_links.append(category_name_link)\n",
    "                article_title_texts.append(article_title_text)\n",
    "                article_title_links.append(article_title_link)\n",
    "\n",
    "#             except:\n",
    "#                 print(page_num, \"error\")\n",
    "                \n",
    "        print(str(page_num) + \" \", end=\"\")\n",
    "\n",
    "#     except:\n",
    "#         print(page_num, \"ERROR\")\n",
    "    \n",
    "    return (category_name_texts, \n",
    "#             category_name_links, \n",
    "            article_title_texts,\n",
    "            article_title_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Defence & Security', 'Education & Employment', 'Health', 'Housing'],\n",
       " ['When can I call an SCDF Ambulance?',\n",
       "  'How can I remain employable?',\n",
       "  \"What's this MERS-CoV that’s been in the news lately?\",\n",
       "  'What HDB grants are there to help?'],\n",
       " ['/factually/content/when-can-i-call-an-scdf-ambulance',\n",
       "  '/factually/content/how-can-i-remain-employable',\n",
       "  '/factually/content/whats-this-merscov-thats-been-in-the-news-lately',\n",
       "  '/factually/content/what-hdb-grants-are-there-to-help'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one out\n",
    "get_articles_from_page(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling with `threading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 9 11 20 7 19 18 10 2 6 14 15 4 17 13 12 16 8 21 3 1 28 24 29 23 25 36 26 22 30 27 37 38 33 41 40 34 31 35 32 42 44 39 43 45 50 48 49 46 47 2.0202012062072754\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as urllib2\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# make the Pool of workers\n",
    "pool = ThreadPool(20) \n",
    "\n",
    "# open the urls in their own threads\n",
    "# and return the results\n",
    "results = pool.map(get_articles_from_page, range(1,51))  # CHANGE THIS\n",
    "\n",
    "# close the pool and wait for the work to finish \n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.shape(results))\n",
    "\n",
    "category_name_texts = []\n",
    "# category_name_links = []\n",
    "article_title_texts = []\n",
    "article_title_links = []\n",
    "\n",
    "for result in results:\n",
    "    category_name_texts += result[:][0]\n",
    "#     category_name_links += result[:][1]\n",
    "    article_title_texts += result[:][1]\n",
    "    article_title_links += result[:][2]\n",
    "\n",
    "links_crawled_2D_array = []\n",
    "\n",
    "for a,b,c in zip(category_name_texts,\n",
    "#                    category_name_links,\n",
    "                   article_title_texts,\n",
    "                   article_title_links):\n",
    "    links_crawled_2D_array.append([a,b,c])\n",
    "\n",
    "my_df = pd.DataFrame(links_crawled_2D_array)\n",
    "my_df.to_csv('links_crawled.csv', index=False, header=[\"category_name_texts\", \n",
    "#                                                        \"category_name_links\",\n",
    "                                                       \"article_title_texts\",\n",
    "                                                       \"article_title_links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category_name_texts                                article_title_texts  \\\n",
      "0  Education & Employment  Are more foreigners taking away jobs of Singap...   \n",
      "1                  Health  Will the Government take back unused MediSave ...   \n",
      "2             Environment  Can the prices in the 1962 Water Agreement be ...   \n",
      "3  Education & Employment  MOM clarifies inaccuracies in TWC2’s articles ...   \n",
      "4                  Health  Did the Government introduce CareShield Life t...   \n",
      "\n",
      "                                 article_title_links  \n",
      "0  /factually/content/are-more-foreigners-taking-...  \n",
      "1  /factually/content/will-the-government-take-ba...  \n",
      "2  /factually/content/can-the-prices-in-the-1962-...  \n",
      "3      /factually/content/mom-clarifies-inaccuracies  \n",
      "4  /factually/content/did-the-government-introduc...  \n",
      "      category_name_texts                                article_title_texts  \\\n",
      "194      Law & Government  Is the Government's procurement process really...   \n",
      "195     Economy & Finance  Why does inflation seem to hit me more than wh...   \n",
      "196           Environment  Would an annual increase in rainfall cause mor...   \n",
      "197  Transport & Motoring  Why is there a need for the Government to inve...   \n",
      "198  Transport & Motoring  Why does the Government need to build the Nort...   \n",
      "\n",
      "                                   article_title_links  \n",
      "194  /factually/content/is-the-governments-procurem...  \n",
      "195  /factually/content/why-does-inflation-seem-to-...  \n",
      "196  /factually/content/would-an-annual-increase-in...  \n",
      "197  /factually/content/why-is-there-a-need-for-the...  \n",
      "198  /factually/content/why-does-the-government-nee...  \n"
     ]
    }
   ],
   "source": [
    "links_crawled = pd.read_csv('links_crawled.csv', header=[0])\n",
    "print(links_crawled.head())\n",
    "print(links_crawled.tail())\n",
    "# headlines = [headline for headline in articles[\"headline\"][:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling for individual articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_crawled_matrix = [links_crawled.iloc[i] for i in range(len(links_crawled.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_name_texts                                      Savings & Taxes\n",
      "article_title_texts    What are the taxes that the Government is cons...\n",
      "article_title_links    /factually/content/what-are-the-taxes-that-the...\n",
      "Name: 20, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# some usage\n",
    "print(links_crawled.iloc[20])\n",
    "# print(links_crawled.iloc[20].name)\n",
    "# print(list(links_crawled[\"article_title_links\"]))\n",
    "# print(links_crawled.as_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_link(link_crawled):\n",
    "    article_link = 'https://www.gov.sg/' + link_crawled[\"article_title_links\"]\n",
    "    print(str(link_crawled.name) + \" \", end=\"\") # id actually\n",
    "\n",
    "    if True:\n",
    "#     try:\n",
    "    #     print(article_link)\n",
    "        page = requests.get(article_link)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        print(soup)\n",
    "\n",
    "        headline_item = soup.find(class_=\"title\")\n",
    "        headline = headline_item.contents[0]  \n",
    "#         print(headline)\n",
    "\n",
    "        date_item = soup.find(class_=\"op-modified\")\n",
    "        date = date_item.text\n",
    "    #     date = date_item.select_one(\"span\").text  # the same\n",
    "#         print(date)\n",
    "\n",
    "        article_text = \"\"\n",
    "        text_soup = soup.find(\"article\")\n",
    "#         print(text_soup)\n",
    "        text_paras = text_soup.select(\"p\")  # searches <p>s, may have issues on some pages\n",
    "        for text_para in text_paras[:-1]:\n",
    "            text_line = text_para.text\n",
    "            article_text += text_line\n",
    "            article_text += \"\\n\"\n",
    "    #     print(article_text)\n",
    "\n",
    "        article_text = unicodedata.normalize(\"NFKD\", article_text)\n",
    "        # https://stackoverflow.com/questions/10993612/python-removing-xa0-from-string\n",
    "\n",
    "        if not headline or not date or not article_text:\n",
    "            print(\"FAULTY\", article_link, \" \" , str(link_crawled.name))\n",
    "\n",
    "#         time.sleep(np.random.rand())\n",
    "        \n",
    "\n",
    "        \n",
    "        return [link_crawled[\"category_name_texts\"],\n",
    "#                 link_crawled[\"category_name_links\"],\n",
    "                link_crawled[\"article_title_texts\"],\n",
    "                link_crawled[\"article_title_links\"],\n",
    "                headline,\n",
    "                date,\n",
    "                article_text]\n",
    "        \n",
    "#     except:\n",
    "#         print(article_link)\n",
    "#         return [link_crawled[\"category_name_texts\"],\n",
    "# #                 link_crawled[\"category_name_links\"],\n",
    "#                 link_crawled[\"article_title_texts\"],\n",
    "#                 link_crawled[\"article_title_links\"],\n",
    "#                 link_crawled[\"article_title_texts\"],\n",
    "#                 date,\n",
    "#                 \"Text not found.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 <html><head>\n",
      "<title>Access Denied</title>\n",
      "</head><body>\n",
      "<h1>Access Denied</h1>\n",
      " \n",
      "You don't have permission to access \"http://www.gov.sg/factually/content/is-it-true-our-public-universities-reserve-20-per-cent-of-their-places-for-foreign-students\" on this server.<p>\n",
      "Reference #18.8563c617.1537036883.af93fad\n",
      "</p></body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'contents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2d6cdffa7de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_info_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_crawled_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-bc79d120d5b5>\u001b[0m in \u001b[0;36mget_info_from_link\u001b[0;34m(link_crawled)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mheadline_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mheadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheadline_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#         print(headline)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'contents'"
     ]
    }
   ],
   "source": [
    "get_info_from_link(links_crawled_matrix[16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl each individual link with `threading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 13 34 7 | FAULTY https://www.gov.sg//factually/content/did-the-public-transport-council-quietly-axe-the-off-peak-pass-for-seniors 52 43   15 22 58  | \n",
      "37 55 16 46 4 25 49 40 1 35 19 31 28 10 | FAULTY https://www.gov.sg//factually/content/three-reasons-why-we-need-to-expand-our-airport-infrastructure  44  13  | \n",
      "14 8 56 53 23 5 26 59 | FAULTY https://www.gov.sg//factually/content/is-it-true-our-public-universities-reserve-20-per-cent-of-their-places-for-foreign-students   16  | \n",
      "17 38 50 47 60 2 63 11 20 29 41 66 69 72 75 | FAULTY https://www.gov.sg//factually/content/do-hdb-flat-buyers-own-their-flat   31  | 78 \n",
      "32 81 84 87 90 | FAULTY https://www.gov.sg//factually/content/is-it-true-that-over-60-per-cent-of-foreign-domestic-workers-here-are-exploited   17 93 96 99  | \n",
      "102 61 64 70 73 105 108 111 76 114 85 117 79 67 | FAULTY82  https://www.gov.sg//factually/content/how-do-i-vote-on-11th-sept-when-i-am-overseas   90  | \n",
      "91 97 88 103 100 65 94 71 62 106 74 86 115 80 77 109 | FAULTY https://www.gov.sg//factually/content/factually-sg50-edition-50-fun-facts-about-out-little-red-dot   111  | \n",
      "112 92 68 118 89 98 120 83 110 123 95 126 101 129 132 107 104 135 138 141 144 147 116 113 150 153 156 159 162 165 168 171 174 177 180 183 186 189 192 195 198 "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'contents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6062a75da7dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# open the urls in their own threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# and return the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_info_from_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinks_crawled_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# close the pool and wait for the work to finish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmapstar\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-3d29513754ff>\u001b[0m in \u001b[0;36mget_info_from_link\u001b[0;34m(link_crawled)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mheadline_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mheadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheadline_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#         print(headline)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'contents'"
     ]
    }
   ],
   "source": [
    "import urllib.request as urllib2\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# make the Pool of workers\n",
    "pool = ThreadPool(20) \n",
    "\n",
    "# open the urls in their own threads\n",
    "# and return the results\n",
    "results = pool.map(get_info_from_link, links_crawled_matrix[:])\n",
    "\n",
    "# close the pool and wait for the work to finish \n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(results)\n",
    "my_df.to_csv('articles.csv', index=False, header=[\"category_name_texts\", \n",
    "#                                                   \"category_name_links\",\n",
    "                                                  \"article_title_texts\",\n",
    "                                                  \"article_title_links\",\n",
    "                                                  \"headline\",\n",
    "                                                  \"date\",\n",
    "                                                  \"article_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category_name_texts                                article_title_texts  \\\n",
      "194      Law & Government  Is the Government's procurement process really...   \n",
      "195     Economy & Finance  Why does inflation seem to hit me more than wh...   \n",
      "196           Environment  Would an annual increase in rainfall cause mor...   \n",
      "197  Transport & Motoring  Why is there a need for the Government to inve...   \n",
      "198  Transport & Motoring  Why does the Government need to build the Nort...   \n",
      "\n",
      "                                   article_title_links  \\\n",
      "194  /factually/content/is-the-governments-procurem...   \n",
      "195  /factually/content/why-does-inflation-seem-to-...   \n",
      "196  /factually/content/would-an-annual-increase-in...   \n",
      "197  /factually/content/why-is-there-a-need-for-the...   \n",
      "198  /factually/content/why-does-the-government-nee...   \n",
      "\n",
      "                                              headline  \\\n",
      "194  \\r\\n                Is the Government's procur...   \n",
      "195  \\r\\n                Why does inflation seem to...   \n",
      "196  \\r\\n                Would an annual increase i...   \n",
      "197  \\r\\n                Why is there a need for th...   \n",
      "198  \\r\\n                Why does the Government ne...   \n",
      "\n",
      "                                                  date  \\\n",
      "194  \\r\\n                    19 Feb 2018\\r\\n       ...   \n",
      "195  \\r\\n                    19 Feb 2018\\r\\n       ...   \n",
      "196  \\r\\n                    19 Feb 2018\\r\\n       ...   \n",
      "197  \\r\\n                    19 Feb 2018\\r\\n       ...   \n",
      "198  \\r\\n                    19 Feb 2018\\r\\n       ...   \n",
      "\n",
      "                                          article_text  \n",
      "194  What is the fastest and most efficient way for...  \n",
      "195  \\n \\n\\n \\nThe CPI provides information on econ...  \n",
      "196  You may have read about PUB’s plans to build a...  \n",
      "197  “Aren’t the bus operators already making profi...  \n",
      "198  The North-South Expressway (NSE) will improve ...  \n"
     ]
    }
   ],
   "source": [
    "articles = pd.read_csv('articles.csv', header=[0])\n",
    "print(articles.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay it seems that I got blocked from spamming the site :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
